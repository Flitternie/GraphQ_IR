{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"./bart-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('./bart-base/tokenizer_config.json',\n './bart-base/special_tokens_map.json',\n './bart-base/vocab.json',\n './bart-base/merges.txt',\n './bart-base/added_tokens.json')"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"<E>\",\"</E>\",\"<A>\",\"</A>\",\"<R>\",\"</R>\",\"<V>\",\"</V>\",\"<Q>\",\"</Q>\",\"<C>\",\"</C>\"])\n",
    "# entity, attribute, relation, value, qualifier, concept\n",
    "tokenizer.save_pretrained(\"./bart-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart2sparql.sparql_engine import get_sparql_answer\n",
    "from utils.load_kb import DataForSPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading CFQ IR parser\n",
    "from IR_CFQ import sparql_parser\n",
    "sparql_parser = reload(sparql_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = DataForSPARQL(os.path.join(\"./dataset_full/\", 'kb.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_program_seq(program):\n",
    "    seq = []\n",
    "    for item in program:\n",
    "        func = item['function']\n",
    "        inputs = item['inputs']\n",
    "        seq.append(func + '(' + '<c>'.join(inputs) + ')')\n",
    "    seq = '<b>'.join(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(\"./dataset_full/train.json\"))\n",
    "train_sparql = [item['sparql'] for item in train_data]\n",
    "train_program = [item['program'] for item in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data\n",
    "train_data = json.load(open(\"./dataset_new/train.json\"))\n",
    "train_sparql = [item['sparql'] for item in train_data]\n",
    "train_program = [item['program'] for item in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = json.load(open(\"./dataset_full/val.json\"))\n",
    "val_sparql = [item['sparql'] for item in val_data]\n",
    "val_program = [item['program'] for item in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open(\"./dataset_full/test.json\"))\n",
    "test_sparql = [item['sparql'] for item in test_data]\n",
    "test_program = [item['program'] for item in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparql avg len: 103.476964\n",
      "program avg len: 52.921400\n"
     ]
    }
   ],
   "source": [
    "# measuring the average sequence length of sparql and program after being tokenized\n",
    "sparql_total_len = 0\n",
    "program_total_len = 0\n",
    "\n",
    "assert len(train_sparql) == len(train_program)\n",
    "for s, p in zip(train_sparql, train_program):\n",
    "    sparql_total_len += len(tokenizer(s)['input_ids'])\n",
    "    program_total_len += len(tokenizer(get_program_seq(p))['input_ids'])\n",
    "\n",
    "print(\"sparql avg len: %f\\nprogram avg len: %f\" % (sparql_total_len / len(train_sparql), program_total_len / len(train_program)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template avg len: 25.130510\n",
      "IR avg len: 39.855832\n"
     ]
    }
   ],
   "source": [
    "# measuring the average sequence length of ir after being tokenized\n",
    "ir_total_len = 0\n",
    "template_total_len = 0\n",
    "train_template = [i[\"origin\"] for i in train_data]\n",
    "\n",
    "assert len(train_ir) == len(train_template)\n",
    "for ir, template in zip(train_ir, train_template):\n",
    "    ir_total_len += len(tokenizer(ir)['input_ids'])\n",
    "    template_total_len += len(tokenizer(template)['input_ids'])\n",
    "\n",
    "print(\"Template avg len: %f\\nIR avg len: %f\" % (template_total_len / len(train_template), ir_total_len / len(train_ir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KqaPro_Parser.program_v2 import ProgramIRTranslator\n",
    "translator = ProgramIRTranslator.IR_translator()\n",
    "# cfq_parser = sparql_parser.KqaParser(train_sparql)\n",
    "for point in train_data:\n",
    "    if \"Manhattan\" in point[\"origin\"]:\n",
    "        print(point[\"origin\"])\n",
    "        print(point[\"rewrite\"])\n",
    "        print(get_program_seq(point[\"program\"]))\n",
    "        print(translator.program_to_ir(point[\"program\"]))\n",
    "        # print(cfq_parser.f_reversible(point[\"sparql\"]))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_func = \"Select\"\n",
    "target = []\n",
    "for i in new_train_data:\n",
    "    s = i['rewrite']\n",
    "    p = i['program']\n",
    "    q = i['sparql']\n",
    "    if program_func in get_program_seq(p):\n",
    "        target.append(p)\n",
    "        if len(target) == 20:\n",
    "            break\n",
    "\n",
    "for t in target:\n",
    "    print(get_program_seq(t))\n",
    "    print(program_to_ir(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [line for line in open(\"./exp_results/UIR/val_predict.txt\")]\n",
    "gold = [line for line in open(\"./exp_results/UIR/val_gold.txt\")]\n",
    "wrong = []\n",
    "test_nl = [item[\"rewrite\"] for item in val_data]\n",
    "for nl, g, p in zip(test_nl, gold, pred):\n",
    "    if g != p:\n",
    "        wrong.append([\"nl: \"+nl.strip(), \"gold: \"+g.strip(),\"pred: \"+p.strip()])\n",
    "print(len(wrong))\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vocab of answer: 81629\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "parser = pickle.load(open(\"./exp_files/CFQ_IR/full/parser.pkl\",'rb'))\n",
    "from utils.data import DataLoader, DistributedDataLoader, prepare_dataset\n",
    "train_dataset, train_vocab = prepare_dataset(\"./exp_files/CFQ_IR/full/vocab.json\", \"./exp_files/CFQ_IR/full/train.pt\", training=True, pretrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load kb\n"
     ]
    }
   ],
   "source": [
    "from bart2program.executor_rule_new import RuleExecutor\n",
    "# from utils.data import load_vocab\n",
    "# vocab = load_vocab(os.path.join(\"./exp_files_new/UIR/full/\", 'vocab.json'))\n",
    "rule_executor = RuleExecutor(os.path.join(\"./dataset_new/\", 'kb.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_executor.entity_name_to_ids['Q7325']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = rule_executor.entities\n",
    "concepts = rule_executor.concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entity = set()\n",
    "unique_attribute = set()\n",
    "unique_relation = set()\n",
    "# unique_qualifier = set()\n",
    "\n",
    "entity_mapping = dict()\n",
    "entity_attribute_mapping = dict()\n",
    "entity_relation_mapping = dict()\n",
    "\n",
    "for name_id, entity in entities.items():\n",
    "    entity_mapping[name_id] = entity[\"name\"]\n",
    "    unique_entity.add(entity[\"name\"])\n",
    "\n",
    "    entity_attribute_mapping[name_id] = set()\n",
    "    entity_relation_mapping[name_id] = set()\n",
    "\n",
    "    for attribute in entity[\"attributes\"]:\n",
    "        unique_attribute.add(attribute[\"key\"])\n",
    "        entity_attribute_mapping[name_id].add(attribute[\"key\"])    \n",
    "    for relation in entity[\"relations\"]:\n",
    "        unique_relation.add(relation[\"predicate\"])\n",
    "        entity_relation_mapping[name_id].add(relation[\"predicate\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16960\n",
      "16960\n",
      "13693\n",
      "629\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "print(len(entity_mapping.keys()))\n",
    "print(len(entity_attribute_mapping.keys()))\n",
    "print(len(unique_entity))\n",
    "print(len(unique_attribute))\n",
    "print(len(unique_relation))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and count repeating entities\n",
    "repeat_entity = dict()\n",
    "for name_id, entity in entity_mapping.items():\n",
    "    if entity in repeat_entity.keys():\n",
    "        repeat_entity[entity][0] += 1\n",
    "        repeat_entity[entity][1].append(name_id)\n",
    "    else:\n",
    "        repeat_entity[entity] = [1, [name_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_concept = set()\n",
    "concept_mapping = dict()\n",
    "concept_relation_mapping = dict()\n",
    "\n",
    "for name_id, concept in concepts.items():\n",
    "    concept_mapping[name_id] = concept[\"name\"]\n",
    "    unique_concept.add(concept[\"name\"])\n",
    "    \n",
    "    concept_relation_mapping[name_id] = set()\n",
    "    try:\n",
    "        for relation in concept[\"relations\"]:\n",
    "            unique_relation.add(relation[\"predicate\"])\n",
    "            concept_relation_mapping[name_id].add(relation[\"predicate\"])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Howard Ashmans\" in unique_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_concept))\n",
    "print(len(unique_relation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_relation:\n",
    "    try:\n",
    "        assert \"{\" not in i and \"}\" not in i\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ir = \"\"\"what is the <Q> for work </Q> of <E> <E> the one that <R> ethnic group </R> backward to <E> Jewish people </E> </E> and <E> John Houseman </E> </E> that <R> nominated for </R> to <E> Academy Award for Best Picture </E>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load kb\n"
     ]
    }
   ],
   "source": [
    "from IR_unified.self_correct import IRCorrector\n",
    "corrector = IRCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Jewish people'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = re.findall(r'\\{([^{}]+)\\}', ir.replace(\"<E>\", \"{\").replace(\"</E>\", \"}\"))\n",
    "entities[0].strip() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Welsh people', 'English people', 'white people']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "difflib.get_close_matches('Jewish people', corrector.unique_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Jewish people'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector.concept_mapping['Q7325']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart2sparql.sparql_engine import get_sparql_answer\n",
    "test_data = json.load(open(\"./dataset_new/test.json\"))\n",
    "test_sparql = [item['sparql'] for item in test_data]\n",
    "\n",
    "gold_answer = []\n",
    "for sparql in test_sparql:\n",
    "    gold_answer = get_sparql_answer(sparql, kb)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python377jvsc74a57bd0e98bfd0d95dd0b437f7ae5c613bc8c0652b208d48ecee3970c89a3d80a03440a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}